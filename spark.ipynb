{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import setupspark\n","setupspark.init()\n","from pyspark.sql import DataFrame\n","from pyspark.sql import SparkSession\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# if not already created automatically, instantiate Sparkcontext\n","spark = SparkSession.builder \\\n","    .appName(\"BigData\") \\\n","    .master(\"spark://127.0.0.1:7077\") \\\n","    .config(\"spark.local.dir\", 'spark_temp') \\\n","    .config(\"spark.worker.cleanup.enabled\", 'true') \\\n","    .config(\"spark.sql.execution.arrow.enabled\", 'true') \\\n","    .config('spark.executor.cores', '8') \\\n","    .config('spark.executor.instances', '1') \\\n","    .config('spark.executor.memory', '4g') \\\n","    .config('spark.dynamicAllocation.enabled', 'false') \\\n","    .config('spark.dynamicAllocation.maxExecutors', '1') \\\n","    .getOrCreate()\n","\n","spark\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_and_split(sp_df, test_size):\n","    from pyspark.ml.feature import VectorAssembler\n","    assembler = VectorAssembler(inputCols = sp_df.columns[:-1], outputCol=\"features\")\n","    features = assembler.transform(sp_df)\n","\n","    # Split the data into train and test\n","    splits = features.randomSplit([1 - test_size, test_size])\n","    train = splits[0]\n","    train.count()   # force loading of data\n","    test = splits[1]\n","    test.count()    # force loading of data\n","\n","    num_classes = sp_df.select(\"defect_type\").distinct().count()\n","    labelCol = sp_df.columns[-1]\n","\n","    return train, test, len(sp_df.columns), num_classes, labelCol\n","\n","def train_evaluate(train, test, hidden_layers, num_columns, num_classes, labelCol):\n","    # specify layers for the neural network:\n","    layers = [num_columns - 1, *hidden_layers, max(2, num_classes)]\n","\n","    # create the trainer and set its parameters\n","    from pyspark.ml.classification import MultilayerPerceptronClassifier\n","    trainer = MultilayerPerceptronClassifier(labelCol=labelCol, maxIter=500, layers=layers)\n","\n","    # train the model\n","    model = trainer.fit(train)\n","\n","    # compute accuracy on the test set\n","    result = model.transform(test)\n","    predictionAndLabels = result.select(\"prediction\", labelCol)\n","\n","    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","    evaluator = MulticlassClassificationEvaluator(labelCol=labelCol, metricName=\"accuracy\")\n","    return evaluator.evaluate(predictionAndLabels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load, arange and split the data\n","test_size = 0.2\n","hidden_layers = (50,100,50)\n","sp_df: DataFrame = spark.read.load(r\"data\\parquets\")\n","# sp_df = sp_df.sample(0.000128)    # take only a sample of the data\n","train, test, num_columns, num_classes, labelCol = read_and_split(sp_df, test_size)\n","\n","# some statistics\n","print(\"Train data count:\", train.count())\n","train.groupBy(\"defect_type\").count().show()\n","print(\"Test data count:\", test.count())\n","test.groupBy(\"defect_type\").count().show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_evaluate(train, test, hidden_layers, num_columns, num_classes, labelCol)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}